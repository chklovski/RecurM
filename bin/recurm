#!/usr/bin/env python

__author__ = "Alex Chklovski"
__version__ = "0.3.0"
__maintainer__ = "Alex Chklovski"
__email__ = "chklovski near gmail.com"
__status__ = "Development"

import argparse
from argparse import RawTextHelpFormatter
import sys
import logging
import tempfile
import pandas as pd
import subprocess
import os
import shutil

sys.path = [os.path.join(os.path.dirname(os.path.realpath(__file__)), '..')] + sys.path

from recurm import clusterer
from recurm import FASTA_manager
from recurm import fileManager
from recurm import mapper
from recurm.defaultValues import DefaultValues




def is_directory(path):
    if os.path.isdir(path):
        return path
    else:
        raise argparse.ArgumentTypeError(f"{path} is not a valid directory")

if __name__ == '__main__':

    num_threads = 1
    min_contig_len = 2500

    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument('--debug', help='output debug information', action="store_true")
    parent_parser.add_argument('--version', help='output version information and quit', action='version',
                               version=__version__)
    parent_parser.add_argument('--quiet', help='only output errors', action="store_true")

    parser = argparse.ArgumentParser(parents=[parent_parser])
    subparsers = parser.add_subparsers(title="Sub-commands", dest='subparser_name',
                                       parser_class=argparse.ArgumentParser)
    subparser_name_to_parser = {}


    def new_subparser(subparsers, parser_name, parser_description):
        subpar = subparsers.add_parser(parser_name,
                                       description=parser_description,
                                       help=parser_description,
                                       formatter_class=RawTextHelpFormatter,
                                       parents=[parent_parser])
        subparser_name_to_parser[parser_name] = subpar
        return subpar


    cluster_description = 'Cluster contigs from metagenome assemblies and identify putative MGEs.'

    cluster_parser = new_subparser(subparsers, 'cluster', cluster_description)

    cluster_arguments = cluster_parser.add_argument_group('required arguments')

    cluster_parser.add_argument('--assemblies_directory', '-i',
                                help="Path to folder containing all metagenomic assemblies to be analyzed",
                                required=True)
    cluster_parser.add_argument('--output', '--output-directory', '-o', help="Path output to folder",
                                required=True)

    cluster_arguments = cluster_parser.add_argument_group('additional arguments')

    cluster_arguments.add_argument('-x', '--extension',
                                   help='Extension of input files. [Default: .fasta]', default='.fasta')

    cluster_arguments.add_argument('--force', action='store_true', help='overwrite output directory [default: do not overwrite]',
                                   default=False)
    #todo: fix this for multiple recurm rounds
    cluster_arguments.add_argument('--threads', '-t', type=int, metavar='num_threads',
                                   help='number of CPUS to use [default: %i]' % num_threads, default=num_threads)

    cluster_arguments.add_argument('--min-contig-len', '--min_contig_len', '-m', type=int, metavar='min_contig_len',
                                   help='Minimum contig length to consider for clustering [default: 2500]',
                                   default=min_contig_len)

    cluster_arguments.add_argument('-c', '--min_cluster_size', '--min-cluster-size',
                                   help='Minimum size of putative cluster. [Default: 3]', default=DefaultValues.MIN_CLUSTER_SIZE)

    cluster_arguments.add_argument('-s', '--size_bins', '--size-bins',
                                   help='Divide contigs into this many size bins for faster RecurM processing. Set to 1 to run RecurM on all assemblies simultaneously [Default: 4]', default=DefaultValues.CONTIG_SIZE_BINS)

    cluster_arguments.add_argument('--resume', action='store_true',
                                   help='Continue from a previous RecurM run to skip steps already done.', default=False)
                                   
    cluster_arguments.add_argument('--keep_related', action='store_true',
                                   help='Do not collapse highly related short clusters into long clusters at the end - keep all clusters [default: Collapse]', default=False)

    cluster_arguments.add_argument('--fast', action='store_true',
                                   help='Change minimap2 heuristics options to stop chaining early, avoiding quadratic runtimes - best for very large datasets (> 1000 assemblies) [default: False]', default=False)

    cluster_arguments.add_argument('--collapse_against_assembly', action='store_true',
                                   help='Recommended: Collapse any shorter clusters into longer contigs (will discard integrated elements) [default: False]', default=False)

    cluster_arguments.add_argument('--keep_inversions', action='store_true',
                                   help='Keep circular alignments representing inversions [default: False]', default=False)

    cluster_arguments.add_argument('--keep_temp_files', action='store_true',
                                   help='Keep temporary files (primarily linear/circular mapping/alignment files) [default: False]', default=False)
 
    cluster_arguments.add_argument('--long', action='store_true',
                                   help='Use options appropriate for long read assemblies (Note: should be at least 50 percent of assemblies; will output imperfect clusters) [default: False, assume short reads; no imperfect clusters]', default=False)

    ###
    merge_cluster_description = 'Identify common cluster from two RecurM runs of the *same* samples assembled using *different* assemblers.'

    merge_cluster_parser = new_subparser(subparsers, 'merge', merge_cluster_description)

    merge_cluster_arguments = merge_cluster_parser.add_argument_group('required arguments')

    merge_cluster_parser.add_argument('--recurm_directories', '-i', type=is_directory, nargs=2,
                                help="Path to two folders containing two RecurM runs",
                                required=True)

    merge_cluster_parser.add_argument('--output', '--output-directory', '-o', help="Path output to folder",
                                required=True)

    merge_cluster_arguments = merge_cluster_parser.add_argument_group('additional arguments')

    merge_cluster_arguments.add_argument('--force', action='store_true',
                                   help='overwrite output directory [default: do not overwrite]',
                                   default=False)

    merge_cluster_arguments.add_argument('--threads', '-t', type=int, metavar='num_threads',
                                   help='number of CPUS to use [default: %i]' % num_threads, default=num_threads)


    merge_cluster_arguments.add_argument('--strict', action='store_true',
                                   help='Keep only circular clusters found by both RecurM runs [default: False]',
                                   default=False)



    if (len(sys.argv) == 1 or sys.argv[1] == '-h' or sys.argv[1] == '--help' or sys.argv[1] == 'help'):
        print('                ...::: RecurM v' + __version__ + ' :::...''')
        print('\n  General usage:')
        print('    cluster         -> %s' % cluster_description)
        print('    merge           -> %s' % merge_cluster_description)
        #        print('    testrun         -> %s' % testrun_description)

        print('\n  Use recurm <command> -h for command-specific help.\n')
        sys.exit(0)

    else:
        args = parser.parse_args()

    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO

    logging.basicConfig(level=loglevel, format='[%(asctime)s] %(levelname)s: %(message)s',
                        datefmt='%m/%d/%Y %I:%M:%S %p')



    if args.subparser_name == 'cluster':
        #        mode = validate_args_model_choice(args)

        # TODO: Check if we got a list of files or a folder. For now assume a single folder with nothing but assemblies

        #make sure output directory is empty and writable
        fileManager.check_empty_dir(os.path.abspath(args.output), args.force)

        assemblies_list = fileManager.list_assembly_folder(args.assemblies_directory, args.extension)
        logging.info("Clustering contigs from {} assemblies with {} threads.".format(len(assemblies_list), args.threads))
        if int(args.size_bins) > 1:
            size_bins = FASTA_manager.determine_size_cutoffs(args.threads, assemblies_list, args.min_contig_len, int(args.size_bins))
        else:
            size_bins = [(args.min_contig_len, 100000000)]
            

        #Set up size bin folders

        logging.info(
            'Concatenating all assemblies into master assembly files. Only contigs of length {} or more bp considered.'.format(
                args.min_contig_len))
        combined_assemblies_list, master_assembly = FASTA_manager.setup_and_format_assemblies(args.min_contig_len, assemblies_list, args.output,
                                                                        size_bins)



        # Run recurM on each size bin separately

        combined_chunk_contigs = os.path.join(args.output, DefaultValues.EXTRACTED_CHUNK_CONTIGS_FILE)

        fileManager.make_sure_path_exists(os.path.join(args.output, DefaultValues.ALIGNMENT_DIR))

        combined_linear_alignments = os.path.join(os.path.abspath(args.output), DefaultValues.ALIGNMENT_DIR, DefaultValues.SECOND_PASS_NAME)
        combined_circular_alignments = os.path.join(os.path.abspath(args.output), DefaultValues.CIRCULAR_ALIGNMENTS_NAME)

        for idx, entry in enumerate(combined_assemblies_list):

            floor = str(entry).split('_')[-3]
            ceiling = str(entry).split('_')[-2]

            logging.info('')
            logging.info('Aligning contigs from chunk {} out of {}: [Min size: {}] [Max size: {}]'.format(idx+1,
                                                                               len(combined_assemblies_list),
                                                                               floor, ceiling))

            clust_process = clusterer.Clusterer(args.min_contig_len, args.min_cluster_size, args.threads,
                                            os.path.abspath(os.path.dirname(entry)), args.extension, args.force, args.resume,
                                            args.keep_related, args.collapse_against_assembly, args.keep_inversions, args.keep_temp_files, args.long)

            clust_process.cluster_all_vs_all(os.path.abspath(entry), master_assembly, args.fast, is_chunk=True)

            circ_file = os.path.join(os.path.dirname(entry), DefaultValues.CIRCULAR_ALIGNMENTS_NAME)
            linear_file = os.path.join(os.path.dirname(entry), DefaultValues.ALIGNMENT_DIR, DefaultValues.SECOND_PASS_NAME)

            #
            logging.info('Adding successfully aligned contigs to contig master file')

            if os.path.isfile(circ_file):

                #get the contig names
                cmd = "cut -f1 {} >> {}; cut -f6 {} >> {}" \
                    .format(circ_file, combined_chunk_contigs, circ_file, combined_chunk_contigs)
                logging.debug(cmd)
                subprocess.call(cmd, shell=True)

                #get actual alignment
                cmd = "cat {} >> {}" \
                    .format(circ_file, combined_circular_alignments)
                logging.debug(cmd)
                subprocess.call(cmd, shell=True)


            if os.path.isfile(linear_file):

                #get contig names
                cmd = "cut -f2 {} >> {}; cut -f7 {} >> {}"\
                    .format(linear_file, combined_chunk_contigs, linear_file, combined_chunk_contigs)
                logging.debug(cmd)
                subprocess.call(cmd, shell=True)

                logging.debug('Finished concatenating hashes using awk and cat')

                #get actual alignment
                cmd = "cat {} >> {}" \
                    .format(linear_file, combined_linear_alignments)
                logging.debug(cmd)
                subprocess.call(cmd, shell=True)

            folder = os.path.dirname(os.path.abspath(entry))
            shutil.rmtree(folder)


        #remove folders
#        for folder in combined_assemblies_list:
#           folder = os.path.dirname(os.path.abspath(folder))
#           shutil.rmtree(folder)

        logging.info('Sorting resultant contig file.')
        cmd = "sort -u --parallel {} {} > {}_tmp && mv {}_tmp {}" \
           .format(args.threads, combined_chunk_contigs, combined_chunk_contigs, combined_chunk_contigs, combined_chunk_contigs)
        logging.debug(cmd)
        subprocess.call(cmd, shell=True)


        logging.info('Creating new assembly file with all contigs passing alignment thresholds.')
        #make a new combined assemblies file from
        contigs_assembly_file = FASTA_manager.create_final_contigs_assembly(combined_chunk_contigs, master_assembly)


        clust_process = clusterer.Clusterer(args.min_contig_len, args.min_cluster_size, args.threads,
                                            os.path.abspath(os.path.dirname(contigs_assembly_file)), args.extension, args.force,
                                            args.resume,
                                            args.keep_related, args.collapse_against_assembly, args.keep_inversions, args.keep_temp_files, args.long)


        clust_process.cluster_all_vs_all(os.path.abspath(contigs_assembly_file), master_assembly, args.fast, is_chunk=False)


    elif args.subparser_name == 'merge':

        fileManager.check_empty_dir(os.path.abspath(args.output), args.force)

        logging.info("Merging multiple RecurM runs and keeping common clusters.")
        logging.info("By default, keeping all circular clusters and only common non-circular clusters.")
        logging.info("Use the --strict command to output only common circular clusters.")

        # Read in all clusters and concatenate them


        master_file_1, master_file_2 = FASTA_manager.concatenate_clusters(args.recurm_directories[0],
                                                                          args.recurm_directories[1],
                                                                          args.output)

        logging.info("Identifying related clusters from previous RecurM runs with {} threads.".format(args.threads))

        mapfile = os.path.join(args.output, 'mapping.tsv')

        merge_mapper = mapper.MergeMapping(args.threads)

        logging.info(
            'Mapping all clusters against each other to identify common clusters.')
        mapfile = merge_mapper.mapMerge(master_file_1, master_file_2, mapfile, args.threads)

        matched_clusters = merge_mapper.process_map(mapfile, args.output)

        #find all shared ID's - separately for circular and non-circular
        logging.info('Determining shared clusters...')

        total_output = FASTA_manager.find_intersecting_clusters(args.recurm_directories[0],args.recurm_directories[1],
                                                 matched_clusters, args.output, args.strict)

        matched_clusters.to_csv(os.path.join(args.output, 'filtered_mapping.tsv'), sep='\t', index=False)



        logging.info('Writing results to file.')
        total_output.to_csv(os.path.join(args.output, 'results', 'cluster_information.tsv'), sep='\t', index=False)








    else:
        raise Exception("Programming error")
